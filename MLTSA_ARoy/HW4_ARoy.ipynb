{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arn97/MLTSA25_ARoy/blob/main/MLTSA_ARoy/HW4_ARoy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWKE7cXHsHCl"
      },
      "source": [
        "\n",
        "This is an exercise on feature extractoin and Gaussian Processes interpolation of sparse time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx-fcjEslVYc"
      },
      "source": [
        "## OVERVIEW\n",
        "\n",
        "The PLAsTiCC challend is a Kaggle challange to classify astrophysical objects (e.g. stars, exploding stars, stars with planets transiting, black holes) based on their time variability in light emission in 6 different bands (i.e. wavelength ranges). The sampling (the cadence of the observations) is designed to show what the upcming Rubin Observatory LSST (Legacy Survey of Space and Time) will see when it starts observing (in 2023).\n",
        "\n",
        "You can refer to this notebook https://www.kaggle.com/michaelapers/the-plasticc-astronomy-starter-kit (written by my good friend Gautham Narayan) to get started and ingest and preprocess the PLASTiCC challenge data. If you are not familiar with classes (object oriented syntax) do not worry about it: I do want you to extract the features yourself. So use it as a guideline, but do not just copy and paste.\n",
        "\n",
        "Your task is to extract features, i.e. representations of the time series, and fit the time series with Gaussian Processes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-05T15:08:27.530325Z",
          "start_time": "2020-04-05T15:08:26.971473Z"
        },
        "id": "-G6oPTITlVYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283c2d0f-a4c9-4f06-b803-82f7984f3307"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pylab as pl\n",
        "\n",
        "%pylab inline\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-05T14:30:57.608296Z",
          "start_time": "2020-04-05T14:30:57.606452Z"
        },
        "id": "w1y1TFCHlVYf"
      },
      "source": [
        "# TASK I :  Data Acquisition\n",
        " You can download the data from kaggle\n",
        " --- https://www.kaggle.com/c/PLAsTiCC-2018\n",
        "\n",
        " The data on zenodo seems to be broken\n",
        " --- https://zenodo.org/record/2539456#.XonrIdNKjOQ\n",
        "\n",
        " You are only interested in the dataset called _plasticc_train_lightcurves.csv.gz_ and the correspondingn metadata _plasticc_train_metadata.csv.gz_ (note: metadata literally means data about the data)\n",
        " or if you get data from Kaggle (preferred) called\n",
        " _training_set.csv_ and _training_set_metadata.csv_\n",
        "\n",
        " The data is also in my drive at the link https://drive.google.com/file/d/1-M_xvSnZG0x26vsOPU4fS9HX3KQNu3y7/view?usp=sharing - from this name you can figure out how to read the data in with the package gdown.\n",
        "\n",
        "**Preferred**: you can get the data from kaggle as (mind you, it will take some time! after downloading you want to _unzip_ the data)\n",
        "\n",
        "`!kaggle competitions download -c PLAsTiCC-2018`\n",
        "\n",
        "but you need to agree to the terms of competition first\n",
        "https://www.kaggle.com/settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3GeSII5fsgF"
      },
      "source": [
        "lcvs = pd.read_csv(\"plasticc_train_lightcurves.csv.gz\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "HGckMxtkgi05",
        "outputId": "c439b715-c8d8-4a77-88aa-5f040cd5043e"
      },
      "source": [
        "lcvs.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ellipsis' object has no attribute 'head'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-33f836459bff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlcvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'head'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WiZeVM42Y0Q"
      },
      "source": [
        "print(\"The shape of light curves dataset is:\", lcvs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQL3NuaD2k8p"
      },
      "source": [
        "#check missing values\n",
        "lcvs.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWugx930lVYg"
      },
      "source": [
        "Read the Kaggle challenge for a description of the data . This \"Starter Kit\" which includes a notebook (written by y good friend Gautham Narayan) may be especially useful\n",
        "https://www.kaggle.com/michaelapers/the-plasticc-astronomy-starter-kit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZyOj9TElVYg"
      },
      "source": [
        "For each object there are 6 time series in 6 different passbands: those are different wavelength ranges over which we observe light (e.g. r is roughtly the wavelengths corresponding to the red portion of the rainbow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-05T15:08:28.281141Z",
          "start_time": "2020-04-05T15:08:28.273051Z"
        },
        "id": "MJy3H0dplVYg"
      },
      "source": [
        "bands = lcvs[\"passband\"].unique()\n",
        "print(\"Passbands (filter) identifiers:\", bands)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-05T15:08:28.299030Z",
          "start_time": "2020-04-05T15:08:28.283127Z"
        },
        "id": "75WQrrI8lVYh"
      },
      "source": [
        "objects = lcvs[\"object_id\"].unique()\n",
        "print(\"There are {} astrophysical simulated objects in my dataset\".format(objects.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USNm_ECcPRSI"
      },
      "source": [
        "metadata = ...\n",
        "metadata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuWokYGTRP8r"
      },
      "source": [
        "print(\"The metadata columns are: \", metadata.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tQRZvDRRafP"
      },
      "source": [
        "print(\"The metadata dataset contains features: \", metadata.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toxu4YKKlVYi"
      },
      "source": [
        "# TASK II : data exploration and visualization - plot some time series\n",
        "Visualize my data to get a sense of what we are working with.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZB5DqYwnVpQ"
      },
      "source": [
        "Plot light curves for 10 objects."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR CODE HERE..."
      ],
      "metadata": {
        "id": "dKCBUaLreWQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot explicitly object 84716"
      ],
      "metadata": {
        "id": "sljsKAQbPZjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR CODE HERE"
      ],
      "metadata": {
        "id": "svYpA7PzURe0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvf0NyI9lVYj"
      },
      "source": [
        "# TASK III: Extract features\n",
        "\n",
        "Feature extraction refers to the extraction of statistical features of the datapoints or the generation of low dimensional representations of the data points that can then be passed to a classifier/regression\n",
        "\n",
        "Extract at least 4 features including at least 1 feature from all three category below\n",
        "\n",
        "The code that won the callenge did a number of smart things, of course. It was a feature based classification scheme, which included Gaussian Processes in the preprocessing. The feature extraction part of the code is here. https://github.com/kboone/avocado/blob/master/avocado/plasticc.py\n",
        "\n",
        "Examples of features you can extract:\n",
        "\n",
        "- **from the original lightcurve**:\n",
        "    - mean,\n",
        "    - standard deviation,\n",
        "    - skewness,\n",
        "    - kurthosis,\n",
        "    - minimum\n",
        "    - maximmum,\n",
        "    - error on the minimum and\n",
        "    - error on the maximum,\n",
        "- **from the standardized lightcurve**:\n",
        "    - median\n",
        "    - slope of a line fit\n",
        "    - intercept of a line fit\n",
        "    - likelihood of the line fit\n",
        "    - likelihood of a quadratic fit\n",
        "    - sign of the quadratic parameter\n",
        "\n",
        "\n",
        "- **from the metadata file**:\n",
        "    - decl; this is one of the positions in the sky variables: important because the position on the sky may determin if it is a galactic or extragalactic object\n",
        "    - ddf_bool ; the kind of survey i.e. the data collection scheme that lead to the time series\n",
        "    - hostgal_photoz\n",
        "    - hostgal_photoz_err\n",
        "    \n",
        "The feature in the metadata files are the hardest ones to decide on because they require domain knowledge.\n",
        "Most of the other features in the metadata are unknown for test objects (only known for the training set)\n",
        "\n",
        "\n",
        "Extract at least 4 features including 1 features from all three category above: original lcv, standardized lcv, metadata. Justify each feature. Coming up with different features than the ones I suggested is very welcome! Remember that if the features are extracted from the time series there will be 4 features for each passbands.\n",
        "\n",
        "Note: we are working with bigg-ish data!\n",
        "First off: every new piece should be developed on a subset of the dataset, then run on all the data only when ready.\n",
        "Still, if you try achieve goals by implementing for loops you are not going to be able to run the notebook for the whole data, or even only 1 batch of the full datase. A good solution is to use the groupby() method in pandas. I used groupby everywhere. Also it may be wise to write out intermediate outputs, so you do not have to loose work if your kernel dies or something similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OU5iAEFlVYk"
      },
      "source": [
        "\n",
        "describe each feature you plan to extract. Briefly indicate why you think it is a valuable feature and what data type is it (e.g. continuous, binary, categorical, if continuous does it have a max or min value or is it unbound?)\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ytS0DzhlVYk"
      },
      "source": [
        "### extract some features from the raw lightcurve -\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjaS_wTClVYl"
      },
      "source": [
        "higher order moments of the distribution can be obtained with the scipy.stats.moment() function which can be applied to a grouby() object\n",
        "note that this way you can apply a function that takes only one argument, the argument over which groupby iterates (the flux here). But moments take 2: the value and the moment Create a function call"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-05T15:08:31.653520Z",
          "start_time": "2020-04-05T15:08:31.493218Z"
        },
        "id": "IbscbPtrlVYk"
      },
      "source": [
        "#example\n",
        "means = lcvs.groupby([\"object_id\", \"passband\"])[[\"flux\"]].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-05T15:08:48.782109Z",
          "start_time": "2020-04-05T15:08:31.676698Z"
        },
        "id": "vfHewV0WlVYl"
      },
      "source": [
        "#example\n",
        "from scipy import stats\n",
        "def m2(x):\n",
        "    return stats.moment(x, moment = 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASPTCZn3lVYl"
      },
      "source": [
        "Even with the same standard deviation two distribution can be very different.\n",
        "Look at the this article for a great demonstation of that! https://heap.io/blog/data-stories/anscombes-quartet-and-why-summary-statistics-dont-tell-the-whole-story\n",
        "\n",
        "I extracted  the max and min, but also their uncertainties because outlier points may be measurement issues, in which case the uncertainty may be large : see the plot you made for 84716!\n",
        "\n",
        "If a distribution has spikes the standard deviation will be larger even if the other datapoints are exactly the same. Save the mean and the max of both original distribution and the standardized one (standardized = mean subtracted and divided by the standard deviation).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdlGzTnHlVYm"
      },
      "source": [
        "Standardize the lcvs and extract some features from the raw lightcurve -\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIHttYe69Pkl"
      },
      "source": [
        "You can do it use  the preprocessing.scale function and groupby, which group by object and band. It does its job, but it is slow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRXCenv6A6Rf"
      },
      "source": [
        "print(\"Mean:\", mean_norm)\n",
        "print(\"Standard Deviation:\", std_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-05T14:54:48.400855Z",
          "start_time": "2020-04-05T14:54:48.399021Z"
        },
        "id": "V36QL4dQlVYn"
      },
      "source": [
        "Note: the median of the standardizeed distribution would convey the location of the \"center of mass\" of the data with respect to the mean\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RMB7NBZlVYo"
      },
      "source": [
        "parametric features: extract some features that come from simple models of the lightcurve, e.g. line fits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO_Ja9j_Xpox"
      },
      "source": [
        "the goodness of fit for these models is also a potential feature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Organization: you can put all your features in a dataframe using merge at each step merging each new feature with the olde ones\n"
      ],
      "metadata": {
        "id": "H38lgih9lVYo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WD4YyRabV69"
      },
      "source": [
        "#example\n",
        "features = features.merge(....., left_index=True, right_index=True)\n",
        "features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaOg1-MZQo11"
      },
      "source": [
        "NOTE: when you merge the metadata features you are going to have to do some gymnastic with the indices: this is because the data has 2 indices columns: the object id and the bandpass id, while the metadata has only 1 index: the object id.\n",
        "\n",
        "You can use the function unstuck - here is an example line of code of what worked for me **but** it kind of depends on how you organized your dataframe so this is not guaranteed to work!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-05T15:26:52.274948Z",
          "start_time": "2020-04-05T15:26:52.144078Z"
        },
        "id": "8yuZTwEGlVYp"
      },
      "source": [
        "allfeatures = features.reset_index().set_index(['object_id','passband']).stack(\n",
        "    ).unstack([1,2])\n",
        "allfeatures.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUo0O0_5WTIF"
      },
      "source": [
        "#EXAMPLE OF A POTENTIAL DATAFRAME OUTPUT\n",
        "allfeatures.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpM3iE2HlVYq"
      },
      "source": [
        "what is the shape of your final feature space?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-05T15:52:47.165508Z",
          "start_time": "2020-04-05T15:52:47.162316Z"
        },
        "id": "rwQfgVzLlVYq"
      },
      "source": [
        "print(\"My feature space is {} features (columns)\".format(allfeatures.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mBRMYFSSlwm"
      },
      "source": [
        "Now make histograms for allfeatures to show their distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT4mitUrj2ft"
      },
      "source": [
        "for i in allfeatures.columns:\n",
        "  pl.figure()\n",
        "  pl.hist(allfeatures[i])\n",
        "  pl.xlabel(i)\n",
        "  pl.ylabel('Frequency')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIHmx_fdlVYq"
      },
      "source": [
        "# TASK III : Gaussian Processes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the time series and fit them with GPs.\n",
        "- use the george package goerge (see class demo https://github.com/fedhere/MLTSA_FBianco/blob/main/CodeExamples/GP_examples.ipynb)\n",
        "\n",
        "- make sure you use the standardized time series : before you fit the GP to it each time series shoud be made to be mean 0 and standard deviation 1 (note - mean across all time stamps! not across all lightcurves). YOu can just do it by hand as lc /= lc.mean() or you can use `sklearn.preprocessing.StandardScaler`\n",
        "\n",
        "- make sure you process the uncertainty as well as the data in the standardization! (divide by the same constant the lighcruve fluxes and its uncertaities)\n",
        "\n",
        "- choose an appropriate kernel (e.g. the composite one we use in class that enables periodicity as well as stockastic variations)\n",
        "\n",
        "- take a subset of the time series, ~10, making sure that they are of different types: the type is mapped to the metadata `target` variable\n",
        "\n",
        "- fit the GP kernerl to each time series band by band\n",
        "\n",
        "- collect the best fit parameters and describe their variance (across the 10 time series) `gp.get_parameter_vector()` - can all the time series be fit with the same kernel parameters? how would you initialize the fit for the whole ensamble?\n",
        "\n"
      ],
      "metadata": {
        "id": "HmvhfncYvSjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHALLENGE (Extra Credit): fit the 2D time series:\n",
        "- create a 2D array for each of the ~10 time series you used earlier: 1 dimension is time the other is wavelength (the bandpass)\n",
        "- fit a 2D kernel to the time series.\n",
        "- show the fits with their uncertainties marginalized along one feature at a time: the fit along the time axis for all 6 bands and the fit along the wavelength axis for a subset of the datapoints.\n",
        "- (try and make a surface plot  that conveys the relevant info effectively, I find it difficult)"
      ],
      "metadata": {
        "id": "RvSu8TAmYjfp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6grL0qluXaI_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}